---
import BaseLayout from "../../../layouts/BaseLayout.astro";
import { Image } from "astro:assets";
import flowchartImage from "../../../images/cadence/flowchart.png";
import screen1Image from "../../../images/cadence/screen1.png";
import drawing1Image from "../../../images/cadence/drawing1.jpg";
import drawing2Image from "../../../images/cadence/drawing2.jpg";
import drawing5Image from "../../../images/cadence/drawing5.jpg";
import figma1Image from "../../../images/cadence/figma-1.png";
import figma2Image from "../../../images/cadence/figma-2.png";
import figma3Image from "../../../images/cadence/figma-3.png";
import figma4Image from "../../../images/cadence/figma-4.png";
import figma5Image from "../../../images/cadence/figma-5.png";
import figmaUi1Image from "../../../images/cadence/figma-ui1.jpg";
import figmaUi2Image from "../../../images/cadence/figma-ui2.jpg";
import developmentImage from "../../../images/cadence/development.jpg";
import testingImage from "../../../images/cadence/testing.jpg";
import final1Image from "../../../images/cadence/final1.png";
import final2Image from "../../../images/cadence/final2.png";
import final3Image from "../../../images/cadence/final3.png";
import final4Image from "../../../images/cadence/final4.png";
import final5Image from "../../../images/cadence/final5.png";
import cadenceLogoImage from "../../../images/cadence/cadence-logo.png";
---

<BaseLayout title="Cadence Fitness · Case Study">
  <section class="introduction">
    <nav class="breadcrumb" aria-label="Breadcrumb">
      <a href="/home">Home</a>
      <span aria-hidden="true">/</span>
      <span aria-current="page">Cadence Fitness</span>
    </nav>
    <h1 class="hero-title">
      Workout logging that tracks <span
        style="whitespace:no-wrap; display:inline-block"
        >the full picture<span class="flourish-period">.</span></span
      >
    </h1>
    <ul class="meta-list meta-list--casestudy meta-list--pills">
      <li>Self-initiated</li>
      <li>2026</li>
      <!-- <li>~7 min read</li> -->
    </ul>

    <div class="hero-wrapper">
      <section class="hero-lead">
        <p>
          <strong>
            A strength training app that captures the context behind each set —
            equipment, variations, rest, and training methods — without slowing
            down logging.
          </strong>
        </p>
        <p>
          I built Cadence to explore a personal problem end-to-end, and to
          deliberately test how AI-augmented prototyping and development can
          shorten the path from concept to a usable product.
        </p>
      </section>
      <section class="card card--role card--compact card--transparent">
        <h2>Role</h2>
        <ul class="role-list">
          <li>Research</li>
          <li>Design</li>
          <li>Content</li>
          <li>Development</li>
          <li>
            Testing <br /><em class="small">(moderated + unmoderated)</em>
          </li>
        </ul>
      </section>
    </div>
    <!-- <section class="card">
      <h2>TL;DR</h2>
      <ul>
        <li>Strength training app for context-aware workout logging</li>
        <li>
          Solves unreliable progress tracking caused by setup & calibration
          differences
        </li>
        <li>Offline-first, low-friction, no feature bloat</li>
        <li>Built using AI-augmented development</li>
        <li>Android closed test (2026)</li>
      </ul>
    </section> -->
    <ul class="tools-list">
      <li>
        <img
          class="tools-list__icon"
          src="/images/tools/figma.png"
          alt="Figma"
        />
      </li>
      <li>
        <img class="tools-list__icon" src="/images/tools/miro.png" alt="Miro" />
      </li>
      <li>
        <img
          class="tools-list__icon"
          src="/images/tools/vscode.png"
          alt="VS Code"
        />
      </li>
      <li>
        <img
          class="tools-list__icon"
          src="/images/tools/react.png"
          alt="React"
        />
      </li>
      <li>
        <img
          class="tools-list__icon"
          src="/images/tools/codex.png"
          alt="Codex"
        />
      </li>
      <li>
        <img
          class="tools-list__icon"
          src="/images/tools/trello.png"
          alt="Trello"
        />
      </li>
    </ul>
  </section>

  <section class="card card-split card-split--2-3 content-block">
    <div>
      <h2 class="small">Problem:</h2>
      <h2>
        <span class="flourish-quote">&ldquo;</span>Why does this feel harder
        than last time?<span class="flourish-quote">&rdquo;</span>
      </h2>
      <p>
        For many regular gym-goers, consistent progression is the goal. Yet
        small variations in machine calibration, setup, range of motion, or rest
        can meaningfully affect workload. While most tracking tools capture load
        and reps, contextual factors are often inconsistently recorded, making
        like-for-like comparison difficult.
      </p>

      <p>
        As a result, numbers alone don&apos;t always provide a reliable picture
        of progress.
      </p>
      <p>
        In addition, some gym tracking tools prioritise feature breadth —
        programmes, social features, gamification — which can introduce friction
        into what is typically a quick, in-the-moment task.
      </p>
      <p>
        Cadence began with a focused question: <br /><strong>
          What would a workout tracking tool look like if contextual clarity and
          speed were treated as primary constraints?
        </strong>
      </p>
      <h3>Validation</h3>
      <p>
        To test whether this problem extended beyond my own experience, I spoke
        with personal trainers and regular gym users across a range of training
        styles.
      </p>
      <p>
        A short survey shared within fitness-focused communities explored how
        people currently track sessions, what they record consistently, and
        where comparisons break down.
      </p>
      <p>While approaches varied, two themes emerged:</p>
      <ul>
        <li>Context is often remembered but not systematically recorded.</li>
        <li>
          Logging speed is valued; additional input fields are tolerated only if
          they clearly improve insight.
        </li>
      </ul>

      <p>
        These conversations reinforced the decision to prioritise contextual
        integrity and efficiency over feature breadth.
      </p>

      <hr class="divider" />
      <h3>Project goals</h3>
      <p>
        From the outset, the project was shaped by a small set of constraints:
      </p><ul>
        <li>Flexible, efficient logging</li>
        <li>Reliable like-for-like comparison</li><li>
          In-the-moment progress indicators
        </li>
        <li>Focused scope — no peripheral features</li><li>
          Offline-first architecture
        </li>
        <li>Low infrastructure complexity</li><li>
          Immediate, practical value
        </li>
      </ul><p>
        These goals acted as guardrails for the first release. Any addition had
        to support faster logging or clearer comparison; broader features were
        intentionally deferred until the core behaviour proved valuable.
      </p>
    </div>
    <div>
      <img src="/case-studies/cadence/exercises.png" alt="Exercise" />
      <p class="caption">
        Equipment calibration and settings can significantly impact exercise
        work rate across sessions. <br /><strong>Image: ChatGPT</strong>
      </p>
    </div>
  </section>
  <section class="image-row image-row--full-bleed">
    <Image
      class="full-width-image"
      src={flowchartImage}
      alt="Flowchart"
      widths={[480, 768, 1024, 1440, 1920]}
      sizes="100vw"
    />
  </section>
  <p class="image-row--caption">Core flows mapped in Miro.</p>
  <section class="card-split card-split--2-3">
    <section class="card">
      <h2>Process</h2>

      <p>
        I followed a deliberately lightweight, iterative process focused on
        testing assumptions in use before investing in refinement or
        documentation.
      </p>

      <p>
        High-level flows were mapped in Miro to clarify the core logging and
        review journeys before any interface decisions were made. These were
        translated into Figma prototypes, where interaction details were refined
        through repeated passes to reduce friction and cognitive load.
      </p>

      <p>
        An offline-first architecture meant all data lived on the device. This
        allowed me to build an installable proof of concept early and test it in
        real gym sessions — both personally and with a small group of users.
        Observing how the product behaved under real workout conditions exposed
        edge cases and usability gaps that would have been difficult to identify
        in static prototypes.
      </p>
      <p>
        Building was used as a learning mechanism: each iteration aimed to
        reduce uncertainty around data structure, interaction speed, and
        contextual clarity before pursuing visual polish.
      </p>
    </section>

    <div class="process-image">
      <Image
        src={screen1Image}
        alt="App development"
        style="height:auto;"
        widths={[480, 768, 1024, 1440, 1920]}
        sizes="100vw"
      />
    </div>
  </section>

  <section class="card content-block t">
    <h2>Concept development</h2>
    <p>
      I began with paper sketches to clarify the underlying structure before
      committing to interface detail.
    </p>

    <p>
      A central decision I made was to introduce a flexible tagging system
      capable of capturing meaningful variation within an exercise — equipment,
      setup differences, rest, and training method — without fragmenting the
      core exercise history. My aim was to preserve comparability while allowing
      nuance.
    </p>

    <p>
      This tagging model shaped both the data structure and the interface.
      Rather than creating multiple rigid exercise variants, I designed the
      system so context could be layered dynamically — enabling richer
      comparison without increasing logging friction.
    </p>

    <p>
      I used interactive prototypes to refine how context was added, surfaced,
      and reviewed within a session, ensuring the structure supported fast input
      as well as meaningful reflection.
    </p>
  </section>
  <div class="image-row image-row--desaturated image-row--3">
    <Image
      src={drawing1Image}
      alt="Process sketch 1"
      widths={[480, 768, 1024, 1440, 1920]}
      sizes="100vw"
    />
    <Image
      src={drawing2Image}
      alt="Process sketch 2"
      style=" width:120px;"
      widths={[480, 768, 1024, 1440, 1920]}
      sizes="100vw"
    />
    <!-- <img src="/images/cadence/drawing3.jpg" alt="Process sketch 3" /> -->
    <Image
      src={drawing5Image}
      alt="Process sketch 3"
      widths={[480, 768, 1024, 1440, 1920]}
      sizes="100vw"
    />
  </div>
  <p class="image-row--caption">
    Early concepts were roughed out on paper.<br /> I designed a tagging system to
    capture the nuance of any given exercise&hellip;
  </p>

  <div
    class="image-row image-row--5 image-row--full-bleed"
    data-lightbox-default="full"
  >
    <Image
      src={figma1Image}
      alt="UX"
      widths={[480, 768, 1024, 1440, 1920]}
      sizes="100vw"
    />
    <Image
      src={figma2Image}
      alt="UX"
      widths={[480, 768, 1024, 1440, 1920]}
      sizes="100vw"
    />
    <Image
      src={figma3Image}
      alt="UX"
      widths={[480, 768, 1024, 1440, 1920]}
      sizes="100vw"
    />
    <Image
      src={figma4Image}
      alt="UX"
      widths={[480, 768, 1024, 1440, 1920]}
      sizes="100vw"
    />
    <Image
      src={figma5Image}
      alt="UX"
      widths={[480, 768, 1024, 1440, 1920]}
      sizes="100vw"
    />
  </div>
  <p class="image-row--caption">
    &hellip;and developed the idea as part of the inital wireframes in Figma.
  </p>

  <section class="image-row image-row--full-bleed image-row--2">
    <!-- <div class="image-row image-row--full" style="height:320px"> -->
    <Image
      src={figmaUi1Image}
      alt="UX"
      widths={[480, 768, 1024, 1440, 1920]}
      sizes="100vw"
    />
    <Image
      src={figmaUi2Image}
      alt="UX"
      widths={[480, 768, 1024, 1440, 1920]}
      sizes="100vw"
    />
    <!-- </div> -->
  </section>

  <p class="image-row--caption">Early-flow interactive prototypes.</p>

  <section class="card content-block">
    <h2>Build</h2>

    <p>Delivery speed was an explicit constraint.</p>

    <p>
      After defining the interaction model, I implemented it quickly and
      iterated in the live build where early assumptions didn’t hold up under
      real workout conditions. Code became an extension of the design process
      rather than a separate phase.
    </p>

    <p>
      OpenAI’s Codex accelerated implementation — generating scaffolding and
      state logic from defined interaction rules — so I could move quickly from
      design decisions to testable installs.
    </p>
    <p>
      The architecture stayed lean: offline-first, local storage, and a tagging
      model that layers context onto stable exercises without fragmenting
      history.
    </p>

    <p>
      Each iteration was tested in real gym sessions and adjusted where logging
      slowed or clarity dropped.
    </p>
  </section>
  <section class="image-row image-row--full-bleed image-row--2">
    <Image
      src={developmentImage}
      alt="App development"
      widths={[480, 768, 1024, 1440, 1920]}
      sizes="100vw"
    />
    <Image
      src={testingImage}
      alt="App testing"
      widths={[480, 768, 1024, 1440, 1920]}
      sizes="100vw"
    />
  </section>
  <p class="image-row--caption">
    Storing data on the device and using AI-augmented development allowed me to
    rapidly iterate and test in real conditions with real users.
  </p>

  <section class="card context-block">
    <h2>Beta testing</h2>
    <p>At the time of writing, Cadence is in closed beta via Google Play.</p>

    <p>
      The focus is behavioural validation rather than feature expansion —
      testing whether contextual logging is sustained over time and whether the
      added inputs feel justified in real gym conditions.
    </p>

    <p>
      Early feedback has highlighted different expectations around starting a
      workout: some users prefer to pre-template sessions, others begin logging
      immediately. The MVP supports both, but the entry paths need clearer
      signposting.
    </p>

    <p>
      The beta is being used to observe real usage patterns before committing to
      broader release or infrastructure decisions.
    </p>
  </section>
  <div class="image-row image-row--5 image-row--full-bleed">
    <Image
      src={final1Image}
      alt="UX"
      widths={[480, 768, 1024, 1440, 1920]}
      sizes="100vw"
    />
    <Image
      src={final2Image}
      alt="UX"
      widths={[480, 768, 1024, 1440, 1920]}
      sizes="100vw"
    />
    <Image
      src={final3Image}
      alt="UX"
      widths={[480, 768, 1024, 1440, 1920]}
      sizes="100vw"
    />
    <Image
      src={final4Image}
      alt="UX"
      widths={[480, 768, 1024, 1440, 1920]}
      sizes="100vw"
    />
    <Image
      src={final5Image}
      alt="UX"
      widths={[480, 768, 1024, 1440, 1920]}
      sizes="100vw"
    />
  </div>
  <p class="image-row--caption">
    Screens from the beta release - in Google Play Store closed testing (as of
    February 2026)
  </p>

  <Image
    class="centered-logo"
    src={cadenceLogoImage}
    alt="Cadence logo"
    width={220}
    height={66}
    sizes="220px"
  />
  <section class="card">
    <h2>Reflection <span class="flourish-ampersand">&</span> next steps</h2>
    <p>
      Cadence began as a focused product exploration, but became equally a test
      of process.
    </p>

    <p>
      I used ChatGPT and Claude as architectural sounding boards to
      pressure-test the tagging model and explore edge cases before committing
      to code. For implementation, I used Codex to translate design decisions
      into working logic, allowing me to validate behaviour under real
      conditions while the product was still lightweight.
    </p>
    <p>
      It reinforced that early design decisions are architectural decisions:
      interaction models directly shape data integrity and future adaptability.
    </p>

    <p>
      Treating prototypes as structural artefacts — not just interaction
      demonstrations — proved particularly valuable. Building early exposed edge
      cases and modelling gaps that static design work would likely have missed.
    </p>
    <p>
      As the beta continues, the priority is learning from real usage patterns
      before expanding scope. Early signals suggest onboarding needs refinement,
      particularly in helping users reach meaningful value within the first 30
      seconds. Any future features will be evaluated against the original
      constraints: contextual clarity, logging speed, and structural coherence.
    </p>

    <p>
      This project reflects a deliberate commitment to building as a way of
      learning — using real products to deepen both craft and systems thinking.
    </p>

    <!-- </section> -->
  </section>
</BaseLayout>
<script type="module" src="/scripts/lightbox.js"></script>
